# -*- coding: utf-8 -*-
"""Berlin_dataset_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NuO5wOiYWdXbfQOfWLJPRgdl8z8ik4V-

YOLO Tree Crown Prediction on Berlin Dataset

Runs YOLOv8 instance segmentation inference on satellite imagery (large rasters or slices),
converts mask predictions to georeferenced polygons, applies post-processing (NMS, overlap removal,
smoothing), and exports cleaned predictions as GeoJSON/GeoPackage with resume capability.

Overview:
    Processes satellite tiles with trained YOLO model, extracts tree crown predictions with
    confidence scores, removes overlapping/duplicate detections, and saves results with
    geospatial coordinates (EPSG:25833).

Key Features:

    - Resume from checkpoint (detects already-processed tiles)
    - Sliding window processing (100m x 100m tiles with 20m overlap)
    - Post-processing: NMS by containment, overlap clipping, polygon smoothing
    - Handles both large rasters (windowed) and pre-sliced images
    - Exports predictions with confidence scores as GeoJSON/GeoPackage

Processing Steps:

    1. Load trained YOLO model weights
    2. Iterate over satellite imagery (large rasters or slices)
    3. Run inference with confidence threshold (0.1)
    4. Convert mask predictions to polygons with world coordinates
    5. Apply NMS by containment (remove nested detections)
    6. Clip overlaps by area (smaller polys take priority)
    7. Smooth geometries (buffer out/in)
    8. Save cleaned predictions per tile + final aggregated result

Output:

    - Per-tile predictions: {window_aoi_code}_pred.geojson
    - Final cleaned predictions: _ALL_PREDICTIONS_CLEANED.geojson
    - Supports resuming interrupted runs

Parameters:

    - CONFIDENCE_THRESHOLD: 0.1
    - NMS_THRESHOLD: 0.3
    - OVERLAP_THRESHOLD: 0.8
    - TILE_SIZE: 100m x 100m
    - OVERLAP: 20m per side
    - TARGET_CRS: EPSG:25833 (UTM Zone 33N)
"""


#from google.colab import drive
import os
import rasterio
import numpy as np
import geopandas as gpd
from shapely.geometry import box
from rasterio.transform import from_bounds
from rasterio.features import shapes
from shapely.geometry import shape as geom_shape
from ultralytics import YOLO
import cv2
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import glob
import collections # <-- ADDED FOR RESUME LOGIC

#drive.mount('/content/drive')

base_directory = r'G:/Meine Ablage/masterthesis\data'
raster_slice_dir = os.path.join(base_directory, '2020_DOP_all')
digitized_dir = os.path.join(base_directory, '2020_DOP_all_predictions')

model_name="no_struct_veg_mask_all_crowns_dice_loss_experiment_lr_0.0025_L0.3_D0.7"
prediction_output_dir = os.path.join(digitized_dir, f"preds_{model_name}")
os.makedirs(prediction_output_dir, exist_ok=True)

# Define all IoU thresholds you want to calculate metrics for
IOU_THRESHOLDS = [0.50]
# Define the single IoU threshold to use for generating the visual plots
PRIMARY_IOU_FOR_PLOTTING = 0.25
# Model confidence threshold for predictions
CONFIDENCE_THRESHOLD = 0.1
NMS_THRESHOLD=0.3
OVERLAP_THRESHOLD=0.8

model_path = os.path.join(base_directory, 'runs', model_name, 'weights', 'best.pt')
model = YOLO(model_path)


def masks_to_polygons(masks, original_bounds, out_shape, scores, min_area=1.0):
    """Converts YOLO masks to a GeoDataFrame of polygons."""
    polygons = []
    properties = []
    transform = from_bounds(*original_bounds, width=out_shape[1], height=out_shape[0])
    for i, mask in enumerate(masks):
        mask_resized = cv2.resize(mask.astype(np.uint8), (out_shape[1], out_shape[0]), interpolation=cv2.INTER_NEAREST)
        for geom, val in shapes(mask_resized, mask=mask_resized, transform=transform):
            poly = geom_shape(geom)
            if poly.area > min_area:
                polygons.append(poly)
                properties.append({'confidence': float(scores[i])})
    gdf = gpd.GeoDataFrame(properties, geometry=polygons)
    return gdf

def remove_highly_overlapping_polygons(
    gdf: gpd.GeoDataFrame,
    overlap_threshold: float = 0.8
) -> gpd.GeoDataFrame:
    """
    Removes polygons that are highly overlapping with a higher-confidence polygon.

    This function iterates through polygons sorted by confidence. For any two
    polygons, if their intersection covers more than the `overlap_threshold`
    of the smaller polygon's area, the one with the lower confidence is removed.

    Args:
        gdf (gpd.GeoDataFrame): A GeoDataFrame of predictions, which should
                                have a 'geometry' and 'confidence' column.
        overlap_threshold (float): The threshold for suppression (e.g., 0.9 for 90%).

    Returns:
        gpd.GeoDataFrame: A new GeoDataFrame with highly overlapping duplicates removed.
    """
    if gdf.empty or len(gdf) < 2:
        return gdf

    # Ensure the gdf is sorted by confidence
    gdf_sorted = gdf.sort_values(by='confidence', ascending=False).reset_index(drop=True)

    # A boolean mask to mark polygons for removal. False means keep, True means remove.
    to_remove = pd.Series([False] * len(gdf_sorted), index=gdf_sorted.index)

    # Use a spatial index for much faster intersection queries
    sindex = gdf_sorted.sindex

    for i in range(len(gdf_sorted)):
        if to_remove[i]:
            continue

        current_poly = gdf_sorted.iloc[i].geometry

        # Find potential candidates for overlap using the spatial index
        possible_matches_index = list(sindex.intersection(current_poly.bounds))

        for j in possible_matches_index:
            # Don't compare a polygon to itself or to one already marked for removal
            # Also, only compare with polygons of lower confidence (j > i)
            if i >= j or to_remove[j]:
                continue

            candidate_poly = gdf_sorted.iloc[j].geometry

            # --- MODIFICATION START ---
            # Instead of a simple .contains(), calculate the overlap fraction.
            intersection_area = current_poly.intersection(candidate_poly).area

            # Since the list is sorted, the candidate_poly (j) will always be the
            # smaller or equal one in terms of confidence. We check its area.
            if candidate_poly.area > 0:
                overlap_fraction = intersection_area / candidate_poly.area

                if overlap_fraction > overlap_threshold:
                    to_remove[j] = True  # Mark the highly overlapping polygon for removal
            # --- MODIFICATION END ---

    # Return the GeoDataFrame keeping only the un-suppressed polygons
    gdf_clean = gdf_sorted[~to_remove].reset_index(drop=True)
    return gdf_clean

def nms_by_containment(
    gdf: gpd.GeoDataFrame,
    containment_threshold: float = 0.7
) -> gpd.GeoDataFrame:
    """
    Performs NMS based on containment (Intersection over Smaller Area).

    This function is designed to remove partial "edge" detections from
    a sliding window. It sorts all polygons by area, from largest to smallest.

    It then iterates and removes any *smaller* polygon ('a') that is highly
    contained within a *larger* polygon ('b'), based on the threshold.

    Example:
    If containment_threshold = 0.9:
    If Polyon 'a' (4m2) is 90% inside Polygon 'b' (10m2),
    (intersection / a.area) > 0.9,
    then Polygon 'a' is removed.

    Args:
        gdf (gpd.GeoDataFrame): A GeoDataFrame with 'geometry'.
        containment_threshold (float): The containment fraction (e.g., 0.9 for 90%).

    Returns:
        gpd.GeoDataFrame: The cleaned GeoDataFrame.
    """
    if gdf.empty or len(gdf) < 2:
        return gdf

    # 1. Calculate area and sort by it, LARGEST first.
    gdf_with_area = gdf.copy()
    gdf_with_area['area'] = gdf_with_area.geometry.area

    # Sort by area, descending (largest first)
    gdf_sorted = gdf_with_area.sort_values(by='area', ascending=False).reset_index(drop=True)

    # A boolean mask to mark polygons for removal. False means keep, True means remove.
    to_remove = pd.Series([False] * len(gdf_sorted), index=gdf_sorted.index)

    # Use a spatial index
    sindex = gdf_sorted.sindex

    for i in range(len(gdf_sorted)):
        if to_remove[i]:
            continue

        current_poly = gdf_sorted.iloc[i].geometry # This is the LARGER polygon

        # Find potential candidates for overlap
        possible_matches_index = list(sindex.intersection(current_poly.bounds))

        for j in possible_matches_index:
            # i >= j: Don't compare to self or a larger poly
            # to_remove[j]: Don't compare to a poly already marked for removal
            if i >= j or to_remove[j]:
                continue

            # This is the SMALLER polygon
            candidate_poly = gdf_sorted.iloc[j].geometry
            candidate_area = gdf_sorted.iloc[j].area

            if candidate_area == 0:
                to_remove[j] = True
                continue

            # --- This is the key change: Calculate containment ---
            intersection_area = current_poly.intersection(candidate_poly).area

            # Calculate how much of the SMALLER poly is inside the LARGER one
            containment_fraction = intersection_area / candidate_area

            if containment_fraction > containment_threshold:
                to_remove[j] = True  # Mark the smaller, contained polygon for removal
            # --------------------------------------------------

    # Return the GeoDataFrame keeping only the un-suppressed polygons
    gdf_clean = gdf_sorted[~to_remove].reset_index(drop=True)

    # Drop the temporary 'area' column
    gdf_clean = gdf_clean.drop(columns=['area'])

    return gdf_clean

def clip_overlaps_by_area(
    gdf: gpd.GeoDataFrame
) -> gpd.GeoDataFrame:
    """
    Resolves geometric overlaps by giving priority to SMALLER polygons.

    It sorts all polygons by area, from smallest to largest.
    For each polygon, it subtracts (clips) any area that overlaps
    with *all* polygons that are smaller than it.

    This prevents "moon-shapes" on small trees, moving the clip
    artifacts to the larger, overlapping canopies.

    Args:
        gdf (gpd.GeoDataFrame): A post-NMS GeoDataFrame with 'geometry'.
                                Must have 'confidence' if you want to
                                break ties (though not implemented here).

    Returns:
        gpd.GeoDataFrame: A new GeoDataFrame with no geometric overlaps.
    """
    if gdf.empty or len(gdf) < 2:
        return gdf

    # --- KEY CHANGE: Add area and sort by it ---
    gdf_with_area = gdf.copy()
    gdf_with_area['area'] = gdf_with_area.geometry.area

    # Sort by area, smallest first.
    gdf_sorted = gdf_with_area.sort_values(by='area', ascending=True).reset_index(drop=True)
    # --- END KEY CHANGE ---

    # Use a spatial index for fast queries
    sindex = gdf_sorted.sindex

    # Create a list to hold the new, clipped geometries
    new_geometries = []

    for i in range(len(gdf_sorted)):
        current_poly = gdf_sorted.iloc[i].geometry

        # Find potential *smaller* polygons that might overlap
        # We only need to check polygons with an index < i
        possible_matches_index = list(sindex.intersection(current_poly.bounds))

        # Keep only the smaller ones (index j < i)
        smaller_matches = [j for j in possible_matches_index if j < i]

        if not smaller_matches:
            # No overlaps with smaller polys, keep it as is
            new_geometries.append(current_poly)
            continue

        # --- This is the key clipping step ---
        # Iteratively subtract the geometries of all smaller overlaps
        for j in smaller_matches:
            # Check for actual intersection before subtracting
            if current_poly.intersects(gdf_sorted.iloc[j].geometry):
                try:
                    # Subtract the *smaller* polygon's geometry (j)
                    # from the *current* (larger) polygon (i)
                    current_poly = current_poly.difference(gdf_sorted.iloc[j].geometry)
                except Exception as e:
                    print(f"Warning: Geometry operation failed (poly {i} and {j}): {e}")
                    pass

        new_geometries.append(current_poly)

    # Create the final GeoDataFrame
    gdf_final = gdf_sorted.copy()
    gdf_final['geometry'] = new_geometries

    # Clean up any empty or invalid geometries that might result from clipping
    gdf_final = gdf_final[~gdf_final.is_empty & gdf_final.is_valid]

    # Drop the temporary 'area' column
    gdf_final = gdf_final.drop(columns=['area'])

    return gdf_final.reset_index(drop=True)

def smooth_polygons(
    gdf: gpd.GeoDataFrame,
    buffer_distance: float = 0.25
) -> gpd.GeoDataFrame:
    """
    Simplifies and rounds polygon geometries by buffering out and then in.

    This also helps clean up any invalid geometries (like self-intersections)
    that may have been created during the clipping step.

    Args:
        gdf (gpd.GeoDataFrame): The GeoDataFrame with polygons to smooth.
        buffer_distance (float): The distance (in CRS units, e.g., meters)
                                 to buffer out and in. A small value like
                                 0.25 (25cm) is good for smoothing.
    Returns:
        gpd.GeoDataFrame: A new GeoDataFrame with smoothed geometries.
    """
    if gdf.empty:
        return gdf

    print(f"  Smoothing {len(gdf)} polygons (buffer={buffer_distance}m)...")

    # 1. First, buffer(0) is a common trick to fix invalid geometries
    # (like self-intersections) that clipping might create.
    gdf_fixed = gdf.copy()
    gdf_fixed['geometry'] = gdf_fixed.geometry.buffer(0)

    # 2. Buffer out and then in to smooth the edges
    gdf_fixed['geometry'] = gdf_fixed.geometry.buffer(buffer_distance).buffer(-buffer_distance)

    # 3. Clean up any tiny/empty geometries this might have created
    gdf_clean = gdf_fixed[~gdf_fixed.is_empty & (gdf_fixed.geometry.area > 0.01)] # Min area 0.01 m^2

    return gdf_clean.reset_index(drop=True)

"""# rasterio window"""


from rasterio.windows import Window
from shapely.geometry import box

IOU_THRESHOLDS = [0.50]
PRIMARY_IOU_FOR_PLOTTING = 0.25
CONFIDENCE_THRESHOLD = 0.1
NMS_THRESHOLD=0.3
OVERLAP_THRESHOLD=0.8
TILE_SIZE_METERS = 100
OVERLAP_METERS = 20
TARGET_CRS = "EPSG:25833"
EXPECTED_WINDOWS_PER_RASTER = 625 # <--- üí° SET THIS TO YOUR KNOWN TOTAL

evaluation_results = []
all_preds_gdfs = []
all_gts_gdfs = []

# --- Find all LARGE rasters to process ---
all_image_paths = glob.glob(os.path.join(raster_slice_dir, "*.jp2"))
all_image_paths.sort() # <-- Ensures a consistent processing order

print(f"\nFound {len(all_image_paths)} total large rasters.")
print(f"Target CRS is set to: {TARGET_CRS}")

rasters_to_process = []
try:
    print("Checking for existing processed files to resume...")
    # Get all prediction files that have already been created
    output_files = glob.glob(os.path.join(prediction_output_dir, "*_pred.geojson"))

    if not output_files:
        print("  ‚ÑπÔ∏è Output directory is empty. Processing all rasters.")
        rasters_to_process = all_image_paths
    else:
        # 1. Count how many window files exist for each base raster
        processed_counts = collections.defaultdict(int)
        for f in output_files:
            fname = os.path.basename(f)
            try:
                # Splits "basename_row_col_pred.geojson" into "basename"
                base_name = fname.rsplit('_', 3)[0]
                processed_counts[base_name] += 1
            except IndexError:
                print(f"  ‚ö†Ô∏è Warning: Could not parse output filename {fname}")
        
        # 2. Build the list of rasters that are NOT complete
        fully_completed_count = 0
        for raster_path in all_image_paths:
            base_name = os.path.splitext(os.path.basename(raster_path))[0]
            count = processed_counts.get(base_name, 0)
            
            if count < EXPECTED_WINDOWS_PER_RASTER:
                rasters_to_process.append(raster_path)
            else:
                fully_completed_count += 1
        
        # 3. Report findings
        print(f"  ‚úÖ Found {fully_completed_count} fully completed rasters (>= {EXPECTED_WINDOWS_PER_RASTER} windows).")
        if not rasters_to_process:
            print("  üéâ All rasters are already fully processed! Nothing to do.")
        else:
            print(f"  ‚ñ∂Ô∏è {len(rasters_to_process)} rasters remain to be processed.")
            print(f"  ‚ÑπÔ∏è Starting with: {os.path.basename(rasters_to_process[0])}")

except Exception as e:
    print(f"  ‚ùå Error checking for resume status: {e}. Processing all {len(all_image_paths)} rasters.")
    rasters_to_process = all_image_paths

# --- Main Processing Loop (Over LARGE RASTERS TO PROCESS) ---
for i, raster_path in enumerate(rasters_to_process):
    large_aoi_code = os.path.splitext(os.path.basename(raster_path))[0]
    print(f"\n--- Processing Raster {i+1}/{len(rasters_to_process)}: {large_aoi_code} ---")

    # --- Open Large Raster and Prepare for Slicing ---
    try:
        with rasterio.open(raster_path) as src:
            transform = src.transform
            rows = src.height
            cols = src.width
            crs = src.crs

            # --- CRS CHECK FOR RASTER
            if crs != TARGET_CRS:
                print(f"   ‚ö†Ô∏è WARNING: Source raster CRS is {crs}, but target is {TARGET_CRS}.")
                print(f"     All outputs will be in the raster's CRS: {crs}")
                ACTIVE_CRS = crs
            else:
                print(f"   ‚úÖ Raster CRS confirmed: {crs}")
                ACTIVE_CRS = TARGET_CRS

            # --- Windowing Parameters ---
            pixel_size_x = transform.a
            pixel_size_y = -transform.e
            tile_size_x = int(TILE_SIZE_METERS / pixel_size_x)
            tile_size_y = int(TILE_SIZE_METERS / pixel_size_y)
            step_x = tile_size_x - int(OVERLAP_METERS / pixel_size_x)
            step_y = tile_size_y - int(OVERLAP_METERS / pixel_size_y)

            row_positions = list(range(0, rows, step_y))
            if row_positions[-1] + tile_size_y < rows:
                row_positions.append(rows - tile_size_y)
            col_positions = list(range(0, cols, step_x))
            if col_positions[-1] + tile_size_x < cols:
                col_positions.append(cols - tile_size_x)

            total_windows = len(row_positions) * len(col_positions)
            print(f"   ‚ÑπÔ∏è Raster will be processed in {total_windows} windows (Tile={tile_size_x}px, Step={step_y}px).")
            
            # --- Sanity check ---
            if total_windows != EXPECTED_WINDOWS_PER_RASTER:
                print(f"  ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è WARNING: Your EXPECTED_WINDOWS_PER_RASTER is {EXPECTED_WINDOWS_PER_RASTER},")
                print(f"  ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è but this raster calculated {total_windows} windows. The resume logic may fail *next* time if this is wrong.")

            window_count = 0

            # --- Inner Loop (Processing each window) ---
            for row_start in row_positions:
                for col_start in col_positions:
                    window_count += 1

                    # Define the window
                    row_end = min(row_start + tile_size_y, rows)
                    col_end = min(col_start + tile_size_x, cols)
                    actual_tile_size_y = row_end - row_start
                    actual_tile_size_x = col_end - col_start

                    if actual_tile_size_x <= 0 or actual_tile_size_y <= 0: continue

                    window = Window(col_start, row_start, actual_tile_size_x, actual_tile_size_y)
                    window_aoi_code = f"{large_aoi_code}_{row_start}_{col_start}"

                    if window_count % 50 == 0 or window_count == 1:
                        print(f"     --- Processing window {window_count}/{total_windows}: {window_aoi_code} ---")

                    try:
                        all_gts_gdfs.append(gpd.GeoDataFrame(geometry=[], crs=ACTIVE_CRS)) # <--- CHANGED

                        # --- Read Window Data & Run Prediction ---
                        tile_data_raw = src.read((1, 2, 3), window=window)
                        image_for_model = np.ascontiguousarray(np.transpose(tile_data_raw, (1, 2, 0)))
                        results = model(image_for_model, imgsz=640, conf=CONFIDENCE_THRESHOLD, verbose=False, iou=NMS_THRESHOLD)

                        # --- Process & Save Predictions ---
                        if results[0].masks is None or len(results[0].masks.data) == 0:
                            if window_count % 50 == 0 or window_count == 1: # Only print for the ones we log
                                print("       ‚ÑπÔ∏è Predictions: 0")
                            gdf_pred = gpd.GeoDataFrame(geometry=[])
                        else:
                            masks = results[0].masks.data.cpu().numpy()
                            scores = results[0].boxes.conf.cpu().numpy()
                            window_bounds_for_mask = rasterio.windows.bounds(window, transform)

                            gdf_pred = masks_to_polygons(
                                masks,
                                original_bounds=window_bounds_for_mask,
                                out_shape=(actual_tile_size_y, actual_tile_size_x),
                                scores=scores
                            )
                            if window_count % 50 == 0 or window_count == 1:
                                print(f"       ‚úÖ Predictions: {len(gdf_pred)}")

                        # --- Set CRS for all cases (empty or not) ---
                        if gdf_pred.empty:
                            gdf_pred = gpd.GeoDataFrame(geometry=[], crs=ACTIVE_CRS)
                        else:
                            gdf_pred.set_crs(ACTIVE_CRS, inplace=True)

                        # Save the prediction for this specific window
                        output_path = os.path.join(prediction_output_dir, f"{window_aoi_code}_pred.geojson")
                        gdf_pred.to_file(output_path, driver='GeoJSON')

                        all_preds_gdfs.append(gdf_pred)

                    except Exception as e:
                        print(f"     ‚ùå ERROR processing window {window_aoi_code}: {e}")
                        all_preds_gdfs.append(gpd.GeoDataFrame(geometry=[], crs=ACTIVE_CRS))

    except Exception as e:
        print(f"   ‚ùå ERROR opening or processing large raster {raster_path}: {e}")

print(f"\n--- Finished processing {len(rasters_to_process)} rasters ---")

# This part will now only aggregate the rasters that were *just* processed.
# To get ALL predictions (new + old), you would need to re-run the glob
print(f"Aggregating results for the {len(rasters_to_process)} rasters processed in this run.")
print(f"Total prediction GeoDataFrames collected: {len(all_preds_gdfs)}")
print(f"Total ground truth GeoDataFrames collected: {len(all_gts_gdfs)}")

if all_preds_gdfs:
    final_all_preds = gpd.GeoDataFrame(pd.concat(all_preds_gdfs, ignore_index=True), crs=ACTIVE_CRS)
    print(f"Total individual predictions from this run: {len(final_all_preds)}")
if all_gts_gdfs:
    final_all_gts = gpd.GeoDataFrame(pd.concat(all_gts_gdfs, ignore_index=True), crs=ACTIVE_CRS)
    print(f"Total individual ground truths from this run: {len(final_all_gts)}")


"""# sliced"""

evaluation_results = []
all_preds_gdfs = []
all_gts_gdfs = []


image_paths = glob.glob(os.path.join(raster_slice_dir, "*.tif"))
print(f"\nFound {len(image_paths)} image slices to process.")

# --- Main Processing Loop ---
for i, raster_path in enumerate(image_paths):
    # Derive the 'aoi_code' (slice name) from the filename
    # e.g., "/.../384_5814.tif" -> "384_5814"
    slice_filename = os.path.basename(raster_path)
    aoi_code = os.path.splitext(slice_filename)[0]

    print(f"\n--- Processing slice {i+1}/{len(image_paths)}: {aoi_code} ---")

    try:
        # --- 1. Load Ground Truth (if it exists) ---
        gt_path = os.path.join(digitized_dir, f"{aoi_code}.gpkg")
        if os.path.exists(gt_path):
            gdf_gt = gpd.read_file(gt_path)
            all_gts_gdfs.append(gdf_gt)
            print(f"  ‚úÖ Loaded {len(gdf_gt)} ground truth polygons.")
        else:
            print(f"  ‚ö†Ô∏è Digitized ground truth not found for {aoi_code}, skipping GT.")
            all_gts_gdfs.append(gpd.GeoDataFrame(geometry=[])) # Append empty to keep lists in sync

        # --- 2. Run Prediction ---
        with rasterio.open(raster_path) as src:
            bounds, shape, crs = src.bounds, (src.height, src.width), src.crs

        results = model(raster_path, imgsz=640, conf=CONFIDENCE_THRESHOLD, verbose=False, iou=NMS_THRESHOLD)

        if results[0].masks is None or len(results[0].masks.data) == 0:
            print("  ‚ÑπÔ∏è No objects detected.")
            gdf_pred = gpd.GeoDataFrame(geometry=[])
        else:
            masks = results[0].masks.data.cpu().numpy()
            scores = results[0].boxes.conf.cpu().numpy()
            gdf_pred = masks_to_polygons(masks, original_bounds=bounds, out_shape=shape, scores=scores)
            print(f"  ‚úÖ Found {len(gdf_pred)} initial predictions.")

        # --- 3. Post-process Predictions ---
        if not gdf_pred.empty:
            gdf_pred.set_crs(crs, inplace=True)

        # --- 4. Save and Collect Predictions ---
        output_path = os.path.join(prediction_output_dir, f"{aoi_code}_pred.gpkg")
        gdf_pred.to_file(output_path, driver='GPKG', layer='predictions')

        all_preds_gdfs.append(gdf_pred)

    except Exception as e:
        print(f"  ‚ùå ERROR processing {aoi_code}: {e}")
        # Append empty dataframes to avoid breaking the final concatenation
        all_preds_gdfs.append(gpd.GeoDataFrame(geometry=[]))
        if not os.path.exists(gt_path): # Only if we haven't already appended
             all_gts_gdfs.append(gpd.GeoDataFrame(geometry=[]))

print("\n--- ‚úÖ All slices processed ---")
POST_PROCESS_IOU_THRESHOLD = 0.5

# --- 5. Final Collation ---
# Combine all individual GeoDataFrames into two main ones
if all_preds_gdfs:
    final_preds_gdf = gpd.GeoDataFrame(pd.concat(all_preds_gdfs, ignore_index=True))
    print(f"Total predictions collected (before NMS): {len(final_preds_gdf)}")

    # Apply Containment Removal / NMS on the *combined* dataframe
    if not final_preds_gdf.empty:
      final_preds_gdf['geometry'] = final_preds_gdf.geometry.simplify(0.5)
      final_preds_gdf = smooth_polygons(final_preds_gdf, buffer_distance=0.7)
      final_preds_gdf = nms_by_containment(final_preds_gdf)
      print(f"Total predictions remaining (after NMS): {len(final_preds_gdf)}")
      final_preds_gdf = clip_overlaps_by_area(final_preds_gdf)
    # ----------------------------------------

    # Optional: Save the *final, cleaned* predictions for the whole AOI
    final_cleaned_output_path = os.path.join(prediction_output_dir, "_ALL_PREDICTIONS_CLEANED.geojson")
    final_preds_gdf.to_file(final_cleaned_output_path, driver="GeoJSON")
    print(f"Saved final cleaned predictions to: {final_cleaned_output_path}")
else:
    print("No predictions were collected.")
    final_preds_gdf = gpd.GeoDataFrame(geometry=[])

# --- 6. Ready for Evaluation ---
print("\nReady for final evaluation.")